#!-*-encoding=utf-8-*-

# 1. 对梯度下降法的理解：【注意：w是一个向量】
# 1.1 从迭代角度理解
# 对于梯度的可求导函数，在w处，使用函数∇f（w）表示从: Rd => R，这是f函数的一个偏导数向量，即：∇ f (w) = (∂ f (w)/∂ w [1],........,∂ f (w)/∂ w [d])
# 梯度下降法是一个重复迭代算法：
#    一开始得到一个w的初始值，也就是说 w(1) = 0向量
#    每一次迭代，用户在现在点的负梯度方向，设定一个步长，这个就是更新步长，而对于每一次的w则有公式：
#    w(t+1) = w(t) - η*∇f（w（t））,即：t+1时刻的w是由t时刻的w基础上，基于更新步长与t时刻w的梯度值的相反数的乘积，计算而来

# 很直观地，因为点的梯度方向是f函数在w(t)处，沿着最大增加比率的方向，所以，基于以上公式，算法是在梯度反向进行小幅度的修正，故而：函数的值是逐渐减小的
# 终究，在经过T此迭代后，算法公式输出平均后的向量： ave(w) =(1/T) {∑(T,t=1)w(t)} 即：从w(1)到w(T)的总和除以T
#   算法公式的输出也可以是最后一个向量w(T)
#   或者是表现最好的向量：argmin f(w(t)) ,即：使得f函数具有最小值时所对应的w(t)
#【注意：求得平均的值是更加具有可用性的，特别是在使用梯度下降法对不可微分函数和随机案例时，更加具有代表性】

# 1.2 从纯数学展开式角度理解
# 使用泰勒展开式估计值,对梯度下降进行移动计算.
# f在w向量的梯度，是对f函数在w向量处的第一纬度泰勒展开式： f (u) ≈ f (w) + （u−w,∇ f (w)）
# 当f函数时凸函数时，这个下边界f函数的近似值，是小于等于真实值的【因为真实值按照切线方向所对应的值，比泰勒展开式的估计值高，这样才能具有凸函数的形式】
# 因此，对于逼近w(t)的w向量，我们可以得到近似式： f (w) ≈ f (w (t) ) + <w − w (t) ,∇ f (w (t) )>
# 从而：我们可以最小化这个f(w)的近似值。
# 但是w向量的近似值可能会是发散的，即：远离w(t)的值。故而我们需要最小化：联合w向量和w(t)向量之间的距离，以及f函数在w(t)处的估计值。
# 如果假设η是控制这两值的权重，那么可以进行假设，得到更新规则为：
# w(t+1) = argmin (1/2)*(||w-w(t)||)*(||w-w(t)||) + η{f(w(t)) + <w-w(t),∇f(w(t))>} 而这个方程的求解，与1.1相同